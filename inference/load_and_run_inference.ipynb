{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b12aea5-deb0-4200-b633-c779e0c8af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install dropbox\n",
    "import ktrain\n",
    "import numpy as np\n",
    "import dropbox\n",
    "import pandas as pd\n",
    "# !pip install tqdm\n",
    "import glob\n",
    "from tqdm import tqdm as prog\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4618041-195d-4caf-aaab-75ceea2d70c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/coler/projects/covid/covid_proj/creds.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATS=['HEALTHY_TALK', 'SICK_TALK']\n",
    "import os\n",
    "token = os.environ[\"DB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9beda5e6-7b7c-42c3-845e-78223b7bd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_path = \"./save_model/models/xlm-roberta-large\"\n",
    "    predictor = ktrain.load_predictor(model_path)\n",
    "    test_lst = [\"I feel terrible\",\"Have you seen the latest Game of Thrones?\"]\n",
    "    probs = predictor.predict_proba(test_lst)\n",
    "    print(probs)\n",
    "    print(predictor.predict(test_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ce5e8ff-a03c-43a9-aa68-eaf1e89ee23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropbox_connect():\n",
    "    \"\"\"Create a connection to Dropbox.\"\"\"\n",
    "\n",
    "    try:\n",
    "        dbx = dropbox.Dropbox(TOKEN)\n",
    "    except AuthError as e:\n",
    "        print('Error connecting to Dropbox with access token: ' + str(e))\n",
    "    return dbx\n",
    "\n",
    "\n",
    "def dropbox_list_files(path):\n",
    "    \"\"\"Return a Pandas dataframe of files in a given Dropbox folder path in the Apps directory.\n",
    "    \"\"\"\n",
    "\n",
    "    dbx = dropbox_connect()\n",
    "\n",
    "    try:\n",
    "        files = dbx.files_list_folder(path).entries\n",
    "        files_list = []\n",
    "        for file in files:\n",
    "            if isinstance(file, dropbox.files.FileMetadata):\n",
    "                metadata = {\n",
    "                    'name': file.name,\n",
    "                    'path_display': file.path_display,\n",
    "                    'client_modified': file.client_modified,\n",
    "                    'server_modified': file.server_modified\n",
    "                }\n",
    "                files_list.append(metadata)\n",
    "\n",
    "        df = pd.DataFrame.from_records(files_list)\n",
    "        return df.sort_values(by='server_modified', ascending=False), files\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error getting list of files from Dropbox: ' + str(e))\n",
    "        \n",
    "def dropbox_download_file(dropbox_file_path, local_file_path):\n",
    "    \"\"\"Download a file from Dropbox to the local machine.\"\"\"\n",
    "\n",
    "    try:\n",
    "        dbx = dropbox_connect()\n",
    "\n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            metadata, result = dbx.files_download(path=dropbox_file_path)\n",
    "            f.write(result.content)\n",
    "    except Exception as e:\n",
    "        print('Error downloading file from Dropbox: ' + str(e))\n",
    "        \n",
    "df, files = dropbox_list_files('/twitter_data')\n",
    "# dropbox_download_file(files[0].path_lower, \"db_twitter_data_processed.zip\")\n",
    "# df,files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b7b6b05-84eb-4ad6-8464-04f93c78d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(fn,\n",
    "           text_col = \"tweet_text\"):\n",
    "    try:\n",
    "        dx = pd.read_csv(fn,low_memory=False)\n",
    "    except Exception as e:\n",
    "        dx = pd.read_csv(fn,low_memory=False,lineterminator='\\n')\n",
    "        \n",
    "    # Drop NAN vals from tweet_text column\n",
    "    dx = dx.dropna(subset=[text_col])\n",
    "    dx = dx.loc[dx[text_col] != \"\",:]\n",
    "    dx = dx.reset_index(drop=True,inplace=False)\n",
    "    return dx\n",
    "        \n",
    "def run_predict(fns,\n",
    "                dest_dir):\n",
    "    # define errors list\n",
    "    err_files = []\n",
    "    \n",
    "    # run apply\n",
    "    for fn in prog(fns,desc=\"Progress running apply: \"):\n",
    "        try:\n",
    "            dx = loader(fn)\n",
    "            dy = pd.concat([dx,pd.DataFrame(predictor.predict_proba(dx.tweet_text.to_list()),columns=CATS)],axis=1)\n",
    "            dy[\"predicted_cat\"] = np.where(dy[CATS[1]] > 0.5,1,0)\n",
    "            dy[\"predicted_cat_text\"] = np.where(dy[CATS[1]] > 0.5,CATS[1],CATS[0])\n",
    "            dy.to_csv(dest_dir.format(fn.split(\"/\")[-1]),index=False)\n",
    "        except Exception as e:\n",
    "            st = traceback.format_exc()\n",
    "            err_files += [(fn,e,st)]\n",
    "            print(e)\n",
    "    return err_files\n",
    "\n",
    "def apply_predictor(\n",
    "    input_dir,\n",
    "    dest_dir\n",
    "    ):\n",
    "    # get file names\n",
    "    fns = glob.glob(input_dir)\n",
    "    errors = run_predict(fns,dest_dir)\n",
    "    return errors\n",
    "    \n",
    "\n",
    "if False:\n",
    "    errors = apply_predictor(\n",
    "        \"./data/original/twitter_data_processed/*.csv\",\n",
    "        \"./data/twitter_data_scored/{}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e63d2569-3aea-4a4e-ae8d-b0914787419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#catch errors\n",
    "orig = [fn.split(\"/\")[-1] for fn in glob.glob(\"./data/original/twitter_data_processed/*.csv\")]\n",
    "scored = [fn.split(\"/\")[-1] for fn in glob.glob(\"./data/twitter_data_scored/*.csv\")]\n",
    "diff = [fn for fn in orig if fn not in scored]\n",
    "fns = [f\"./data/original/twitter_data_processed/{fn}\" for fn in diff]\n",
    "if False:\n",
    "    errors_two = run_predict(fns,\"./data/twitter_data_scored/{}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567d754-9a38-472a-af9d-e7fcab6635f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = [fn.split(\"/\")[-1] for fn in glob.glob(\"./data/original/twitter_data_processed/*.csv\")]\n",
    "num_tweets = [loader(f\"./data/original/twitter_data_processed/{fn}\").shape[0] for fn in orig]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e750890-65c1-48a9-9847-1cf887de9b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'169,233,509'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{sum(num_tweets):,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af9f0bb5-b1db-4c6c-895e-1f8bf6d1fc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"All files processed: {len(fns) == 0 and len(scored) == len(orig)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee9786f2-67ac-4ea7-86d7-2ffee7acb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Aggregate by country, nuts_2_region, week, \n",
    "def load_data(\n",
    "    in_dir\n",
    "):\n",
    "    scored_fns = [fn for fn in glob.glob(in_dir)]\n",
    "    df = pd.DataFrame()\n",
    "    for fn in prog(scored_fns,desc=f\"Progress loading data for {in_dir.split('/')[-1].split('*')[0]}: \"):\n",
    "        in_dat = loader(fn)\n",
    "        date = \"-\".join(fn.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-3:])\n",
    "        in_dat[\"date\"] = pd.to_datetime(date,format=\"%Y-%m-%d\")\n",
    "        in_dat[\"week\"] = in_dat.date.dt.isocalendar().week.apply(lambda w: f\"W{w}\")\n",
    "        in_dat[\"year\"] = in_dat.date.dt.year\n",
    "        df = pd.concat([df,in_dat])\n",
    "    return df \n",
    "        \n",
    "def aggregate(in_dir=\"./data/twitter_data_scored/*.csv\",\n",
    "              out_dir=\"./data/twitter_data_aggregated/{}.csv\"):\n",
    "    fns = [fn for fn in glob.glob(in_dir)]\n",
    "    countries = list(set(\"_\".join(fn.split(\"/\")[-1].split(\"_\")[:-4]) for fn in fns))\n",
    "    in_dir_ct = in_dir.replace(\"*\",\"{}*\")\n",
    "    for ct in countries:\n",
    "        dat = load_data(in_dir = in_dir_ct.format(ct))\n",
    "        dat[\"country\"] = ct\n",
    "        agg_dat = dat.groupby([\"country\",\"nuts_2_region\",\"year\",\"week\"]).\\\n",
    "            agg(sick_talk_proportion=('predicted_cat', 'mean'),\n",
    "                num_tweets=('tweet_text', 'count')).\\\n",
    "            reset_index(drop=False,inplace=False)\n",
    "        agg_dat.to_csv(out_dir.format(ct),index=False)\n",
    "        gc.collect()\n",
    "        del dat \n",
    "        del agg_dat\n",
    "\n",
    "if False:\n",
    "    aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f0036646-a0d4-4182-950c-29e86a5015ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "dir_name = \"twitter_data_scored\"\n",
    "dir_name_agg = \"twitter_data_aggregated\"\n",
    "fp = \"./data/{}\"\n",
    "\n",
    "if False:\n",
    "    shutil.make_archive(fp.format(dir_name_agg), 'zip', fp.format(dir_name_agg))\n",
    "    shutil.make_archive(fp.format(dir_name), 'zip', fp.format(dir_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c091746c-07d7-4b9b-a153-07c0c6ade333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading via session\n",
      "FileMetadata(client_modified=datetime.datetime(2023, 8, 3, 19, 11, 23), content_hash='0d8f4c4e677474259e122e639a52b1b50ab1b06899896d7a67cbffe66026a6f5', export_info=NOT_SET, file_lock_info=NOT_SET, has_explicit_shared_members=NOT_SET, id='id:gl16Bl3R7zwAAAAAAAAAHQ', is_downloadable=True, media_info=NOT_SET, name='twitter_data_scored.zip', parent_shared_folder_id=NOT_SET, path_display='/twitter_data/twitter_data_scored.zip', path_lower='/twitter_data/twitter_data_scored.zip', preview_url=NOT_SET, property_groups=NOT_SET, rev='602098c0f09db68e90343', server_modified=datetime.datetime(2023, 8, 3, 19, 11, 28), sharing_info=NOT_SET, size=20449001289, symlink_info=NOT_SET)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "def dropbox_upload(local_path, local_file, dropbox_file_path):\n",
    "    \"\"\"Upload a file from the local machine to a path in the Dropbox app directory.\n",
    "\n",
    "    Args:\n",
    "        local_path (str): The path to the local file.\n",
    "        local_file (str): The name of the local file.\n",
    "        dropbox_file_path (str): The path to the file in the Dropbox app directory.\n",
    "\n",
    "    Example:\n",
    "        dropbox_upload_file('.', 'test.csv', '/stuff/test.csv')\n",
    "\n",
    "    Returns:\n",
    "        meta: The Dropbox file metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        CHUNK_SIZE = 50 * 1024 * 1024\n",
    "        dbx = dropbox_connect()\n",
    "        local_file_path = pathlib.Path(local_path) / local_file\n",
    "        file_size = os.path.getsize(local_file_path)\n",
    "\n",
    "        with open(local_file_path, \"rb\") as f:\n",
    "            if file_size <= CHUNK_SIZE:\n",
    "                meta = dbx.files_upload(f.read(), dropbox_file_path, mode=dropbox.files.WriteMode(\"overwrite\"))\n",
    "            else:\n",
    "                print(\"Uploading via session\")\n",
    "                upload_session_start_result = dbx.files_upload_session_start(f.read(CHUNK_SIZE))\n",
    "                cursor = dropbox.files.UploadSessionCursor(session_id=upload_session_start_result.session_id,\n",
    "                                                          offset=f.tell())\n",
    "                commit = dropbox.files.CommitInfo(path=dropbox_file_path)\n",
    "\n",
    "                while f.tell() < file_size:\n",
    "                    if ((file_size - f.tell()) <= CHUNK_SIZE):\n",
    "                        print(dbx.files_upload_session_finish(f.read(CHUNK_SIZE),\n",
    "                                                       cursor,\n",
    "                                                       commit))\n",
    "                    else:\n",
    "                        dbx.files_upload_session_append(f.read(CHUNK_SIZE),\n",
    "                                                       cursor.session_id,\n",
    "                                                       cursor.offset)\n",
    "                        cursor.offset = f.tell()\n",
    "    except Exception as e:\n",
    "        print('Error uploading file to Dropbox: ' + str(e))\n",
    "\n",
    "if True:\n",
    "    fns = [\"twitter_data_aggregated.zip\",\"twitter_data_scored.zip\"]\n",
    "    for fn in fns:\n",
    "        dropbox_upload(\"./data\",fn,f\"/twitter_data/{fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1fea3-775f-4429-80a8-680fcffdefcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amyl_app]",
   "language": "python",
   "name": "conda-env-amyl_app-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
